{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Feedforward Neural Network with Keras\n",
        "\n",
        "This notebook demonstrates building, training, and evaluating a feedforward neural network using Keras for MNIST digit classification.\n",
        "\n",
        "## Objectives:\n",
        "- Build a feedforward neural network using Keras\n",
        "- Configure training parameters\n",
        "- Train the model on MNIST data\n",
        "- Evaluate performance and visualize results\n",
        "- Understand training metrics and loss curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Libraries imported successfully!\n",
            "TensorFlow version: 2.19.0\n",
            "Keras version: 3.9.2\n",
            "GPU Available: []\n",
            "Number of GPUs: 0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, metrics\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "\n",
        "# Check if GPU is available\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Load and Preprocess Data\n",
        "\n",
        "Let's load the MNIST dataset and prepare it for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📥 Loading MNIST dataset...\n",
            "✅ MNIST dataset loaded!\n",
            "Training data shape: (60000, 28, 28)\n",
            "Test data shape: (10000, 28, 28)\n",
            "🔄 Preprocessing data...\n",
            "Preprocessed training data shape: (60000, 784)\n",
            "Preprocessed training labels shape: (60000, 10)\n",
            "Input dimension: 784\n",
            "Number of classes: 10\n",
            "Training set size: 54000\n",
            "Validation set size: 6000\n",
            "Test set size: 10000\n",
            "✅ Data preprocessing completed!\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "print(\"📥 Loading MNIST dataset...\")\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"✅ MNIST dataset loaded!\")\n",
        "print(f\"Training data shape: {x_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")\n",
        "\n",
        "# Preprocess the data\n",
        "print(\"🔄 Preprocessing data...\")\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten images for feedforward network (28x28 -> 784)\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_onehot = to_categorical(y_train, 10)\n",
        "y_test_onehot = to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Preprocessed training data shape: {x_train_flat.shape}\")\n",
        "print(f\"Preprocessed training labels shape: {y_train_onehot.shape}\")\n",
        "print(f\"Input dimension: {x_train_flat.shape[1]}\")\n",
        "print(f\"Number of classes: {y_train_onehot.shape[1]}\")\n",
        "\n",
        "# Create validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    x_train_flat, y_train_onehot, test_size=0.1, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {x_train_split.shape[0]}\")\n",
        "print(f\"Validation set size: {x_val_split.shape[0]}\")\n",
        "print(f\"Test set size: {x_test_flat.shape[0]}\")\n",
        "\n",
        "print(\"✅ Data preprocessing completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Build the Feedforward Neural Network\n",
        "\n",
        "Let's create a multi-layer feedforward neural network using Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏗️ Building the neural network...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "k:\\Python 3.10\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">615,440</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer0 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │       \u001b[38;5;34m615,440\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_1 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_2 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_3 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,874</span> (4.51 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,182,874\u001b[0m (4.51 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,874</span> (4.51 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,182,874\u001b[0m (4.51 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) for `plot_model` to work.\n",
            "✅ Neural network built successfully!\n"
          ]
        }
      ],
      "source": [
        "# Build the feedforward neural network\n",
        "print(\"🏗️ Building the neural network...\")\n",
        "\n",
        "# Clear any previous models\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Hidden layer 1 (first layer with input shape)\n",
        "    layers.Dense(512, activation='relu', input_shape=(784,), name='hidden_1'),\n",
        "    layers.Dropout(0.2),  # Dropout for regularization\n",
        "    \n",
        "    # Hidden layer 2\n",
        "    layers.Dense(256, activation='relu', name='hidden_2'),\n",
        "    layers.Dropout(0.2),\n",
        "    \n",
        "    # Hidden layer 3\n",
        "    layers.Dense(128, activation='relu', name='hidden_3'),\n",
        "    layers.Dropout(0.1),\n",
        "    \n",
        "    # Output layer (10 neurons for 10 classes)\n",
        "    layers.Dense(10, activation='softmax', name='output_layer')\n",
        "])\n",
        "\n",
        "# Display model architecture\n",
        "print(\"✅ Model architecture:\")\n",
        "model.summary()\n",
        "\n",
        "# Visualize model architecture (optional)\n",
        "try:\n",
        "    tf.keras.utils.plot_model(\n",
        "        model,\n",
        "        to_file='model_architecture.png',\n",
        "        show_shapes=True,\n",
        "        show_layer_names=True,\n",
        "        rankdir='TB',\n",
        "        dpi=150\n",
        "    )\n",
        "    print(\"📊 Model diagram saved as 'model_architecture.png'\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not save model diagram: {e}\")\n",
        "\n",
        "print(\"✅ Neural network built successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Compile and Configure the Model\n",
        "\n",
        "Now let's compile the model with appropriate loss function, optimizer, and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚙️ Compiling the model...\n",
            "✅ Model compiled successfully!\n",
            "\n",
            "Model configuration:\n",
            "Optimizer: Adam (lr=0.001)\n",
            "Loss function: Categorical Crossentropy\n",
            "Metrics: Accuracy, Top-3 Accuracy\n",
            "✅ Callbacks configured:\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "print(\"⚙️ Compiling the model...\")\n",
        "\n",
        "# Import the correct metrics\n",
        "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.001),  # Adam optimizer\n",
        "    loss='categorical_crossentropy',                 # For multi-class classification\n",
        "    metrics=['accuracy', TopKCategoricalAccuracy(k=3, name='top_3_accuracy')]  # Track accuracy and top-3 accuracy\n",
        ")\n",
        "\n",
        "print(\"✅ Model compiled successfully!\")\n",
        "print(\"\\nModel configuration:\")\n",
        "print(f\"Optimizer: Adam (lr=0.001)\")\n",
        "print(f\"Loss function: Categorical Crossentropy\")\n",
        "print(f\"Metrics: Accuracy, Top-3 Accuracy\")\n",
        "\n",
        "# Define callbacks (simplified to avoid potential issues)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"✅ Callbacks configured (simplified to avoid issues)\")\n",
        "print(\"   - Early stopping on validation loss\")\n",
        "print(\"   - Learning rate reduction on plateau\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Train the Model\n",
        "\n",
        "Now let's train our feedforward neural network on the MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting training...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8424 - loss: 0.4966 - top_3_accuracy: 0.9469\n",
            "Epoch 1: val_accuracy improved from -inf to 0.95483, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 133ms/step - accuracy: 0.8426 - loss: 0.4961 - top_3_accuracy: 0.9470 - val_accuracy: 0.9548 - val_loss: 0.1515 - val_top_3_accuracy: 0.9957 - learning_rate: 0.0010\n",
            "Epoch 2/50\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"🚀 Starting training...\")\n",
        "\n",
        "# Check data shapes before training\n",
        "print(f\"Training data shape: {x_train_split.shape}\")\n",
        "print(f\"Training labels shape: {y_train_split.shape}\")\n",
        "print(f\"Validation data shape: {x_val_split.shape}\")\n",
        "print(f\"Validation labels shape: {y_val_split.shape}\")\n",
        "\n",
        "try:\n",
        "    # Try training with callbacks first\n",
        "    history = model.fit(\n",
        "        x_train_split, y_train_split,\n",
        "        epochs=50,\n",
        "        batch_size=128,\n",
        "        validation_data=(x_val_split, y_val_split),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"✅ Training completed with callbacks!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Training with callbacks failed: {e}\")\n",
        "    print(\"🔄 Trying simple training without callbacks...\")\n",
        "    \n",
        "    # Fallback: Simple training without callbacks\n",
        "    history = model.fit(\n",
        "        x_train_split, y_train_split,\n",
        "        epochs=20,  # Reduced epochs for safety\n",
        "        batch_size=128,\n",
        "        validation_data=(x_val_split, y_val_split),\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"✅ Training completed (simple mode)!\")\n",
        "\n",
        "# Save the training history\n",
        "try:\n",
        "    training_history = pd.DataFrame(history.history)\n",
        "    training_history.to_csv('training_history.csv', index=False)\n",
        "    print(\"📊 Training history saved to 'training_history.csv'\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not save training history: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 5: Evaluate the Model and Visualize Results\n",
        "\n",
        "Let's evaluate our trained model and create comprehensive visualizations of the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model on test set\n",
        "print(\"📊 Evaluating model performance...\")\n",
        "\n",
        "test_results = model.evaluate(x_test_flat, y_test_onehot, verbose=0)\n",
        "test_loss = test_results[0]\n",
        "test_accuracy = test_results[1]\n",
        "test_top3_accuracy = test_results[2] if len(test_results) > 2 else None\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "if test_top3_accuracy is not None:\n",
        "    print(f\"Test Top-3 Accuracy: {test_top3_accuracy:.4f}\")\n",
        "else:\n",
        "    print(\"Top-3 Accuracy: Not available\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = model.predict(x_test_flat, verbose=0)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Training & Validation Loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Training & Validation Accuracy\n",
        "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=range(10), yticklabels=range(10), ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Predicted Label')\n",
        "axes[1, 0].set_ylabel('True Label')\n",
        "\n",
        "# Per-class accuracy\n",
        "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "axes[1, 1].bar(range(10), class_accuracy, color='skyblue', edgecolor='navy', alpha=0.7)\n",
        "axes[1, 1].set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Digit Class')\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "axes[1, 1].set_xticks(range(10))\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "for i, acc in enumerate(class_accuracy):\n",
        "    axes[1, 1].text(i, acc + 0.01, f'{acc:.3f}', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Training history and evaluation metrics visualized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual labels\n",
        "def show_predictions(n_samples=16):\n",
        "    \"\"\"Display predictions vs actual labels for sample images\"\"\"\n",
        "    indices = np.random.choice(len(x_test), n_samples, replace=False)\n",
        "    \n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "    fig.suptitle('Predictions vs Actual Labels', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for i, idx in enumerate(indices):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        \n",
        "        # Get prediction\n",
        "        pred_proba = y_pred_proba[idx]\n",
        "        pred_label = np.argmax(pred_proba)\n",
        "        actual_label = y_test[idx]\n",
        "        confidence = pred_proba[pred_label]\n",
        "        \n",
        "        # Plot image\n",
        "        axes[row, col].imshow(x_test[idx], cmap='gray')\n",
        "        \n",
        "        # Set title with prediction info\n",
        "        color = 'green' if pred_label == actual_label else 'red'\n",
        "        axes[row, col].set_title(\n",
        "            f'Actual: {actual_label}\\nPred: {pred_label} ({confidence:.2f})',\n",
        "            color=color, fontsize=10\n",
        "        )\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_predictions()\n",
        "\n",
        "# Show some misclassified examples\n",
        "def show_misclassified(n_samples=12):\n",
        "    \"\"\"Display misclassified examples\"\"\"\n",
        "    misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "    indices = np.random.choice(misclassified_indices, n_samples, replace=False)\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
        "    fig.suptitle('Misclassified Examples', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for i, idx in enumerate(indices):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        \n",
        "        pred_proba = y_pred_proba[idx]\n",
        "        pred_label = np.argmax(pred_proba)\n",
        "        actual_label = y_test[idx]\n",
        "        confidence = pred_proba[pred_label]\n",
        "        \n",
        "        axes[row, col].imshow(x_test[idx], cmap='gray')\n",
        "        axes[row, col].set_title(\n",
        "            f'True: {actual_label} | Pred: {pred_label}\\nConf: {confidence:.2f}',\n",
        "            color='red', fontsize=10\n",
        "        )\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if len(np.where(y_pred != y_test)[0]) > 0:\n",
        "    show_misclassified()\n",
        "else:\n",
        "    print(\"🎉 No misclassifications found!\")\n",
        "\n",
        "print(\"✅ Prediction visualization completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "🎉 **Week 1 Tasks Completed Successfully!**\n",
        "\n",
        "### What we accomplished:\n",
        "\n",
        "1. **Environment Setup**: Configured Python, Jupyter, TensorFlow, and Keras\n",
        "2. **Neural Network from Scratch**: Implemented a basic neural network using only NumPy\n",
        "3. **Data Handling**: Loaded and preprocessed the MNIST dataset\n",
        "4. **Feedforward Neural Network**: Built and trained a multi-layer FNN with Keras\n",
        "5. **Training & Evaluation**: Trained the model and visualized comprehensive results\n",
        "\n",
        "### Key Learning Points:\n",
        "\n",
        "- **Forward Propagation**: How data flows through neural network layers\n",
        "- **Backpropagation**: How gradients are computed and weights updated\n",
        "- **Data Preprocessing**: Normalization, reshaping, and one-hot encoding\n",
        "- **Model Architecture**: Dense layers, activation functions, and dropout\n",
        "- **Training Process**: Loss functions, optimizers, and callbacks\n",
        "- **Evaluation Metrics**: Accuracy, confusion matrix, and classification reports\n",
        "\n",
        "### Performance Achieved:\n",
        "- Our feedforward neural network achieved high accuracy on MNIST digit classification\n",
        "- The model successfully learned to distinguish between all 10 digit classes\n",
        "- Training curves show proper convergence without overfitting\n",
        "\n",
        "This solid foundation prepares us for more advanced deep learning concepts in upcoming weeks! 🚀\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 6: Model Interpretation and Advanced Analysis\n",
        "\n",
        "Let's dive deeper into understanding what our model has learned and its performance characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced model analysis\n",
        "print(\"🔬 Performing advanced model analysis...\")\n",
        "\n",
        "# 1. Analyze prediction confidence\n",
        "confidence_scores = np.max(y_pred_proba, axis=1)\n",
        "predicted_classes = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Confidence distribution\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(confidence_scores, bins=50, alpha=0.7, edgecolor='black', color='skyblue')\n",
        "plt.title('Prediction Confidence Distribution')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Number of Predictions')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Confidence by class\n",
        "plt.subplot(1, 3, 2)\n",
        "class_confidence = [confidence_scores[predicted_classes == i] for i in range(10)]\n",
        "plt.boxplot(class_confidence, labels=range(10))\n",
        "plt.title('Confidence by Predicted Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Confidence Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy vs Confidence\n",
        "plt.subplot(1, 3, 3)\n",
        "correct_predictions = (predicted_classes == y_test)\n",
        "confidence_bins = np.linspace(0, 1, 11)\n",
        "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
        "bin_accuracy = []\n",
        "\n",
        "for i in range(len(confidence_bins)-1):\n",
        "    mask = (confidence_scores >= confidence_bins[i]) & (confidence_scores < confidence_bins[i+1])\n",
        "    if np.sum(mask) > 0:\n",
        "        bin_accuracy.append(np.mean(correct_predictions[mask]))\n",
        "    else:\n",
        "        bin_accuracy.append(0)\n",
        "\n",
        "plt.plot(bin_centers, bin_accuracy, 'o-', linewidth=2, markersize=8)\n",
        "plt.plot([0, 1], [0, 1], 'r--', alpha=0.7, label='Perfect Calibration')\n",
        "plt.title('Reliability Diagram')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Analyze model weights and activations\n",
        "print(\"\\n🧠 Analyzing model internals...\")\n",
        "\n",
        "# Get weights from first layer (input -> hidden)\n",
        "first_layer_weights = model.layers[0].get_weights()[0]  # Shape: (784, 512)\n",
        "\n",
        "# Visualize some learned features\n",
        "n_features_to_show = 16\n",
        "feature_indices = np.random.choice(first_layer_weights.shape[1], n_features_to_show, replace=False)\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "fig.suptitle('Learned Features (First Layer Weights)', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, feature_idx in enumerate(feature_indices):\n",
        "    row = i // 4\n",
        "    col = i % 4\n",
        "    \n",
        "    # Reshape weight vector to 28x28 image\n",
        "    feature_map = first_layer_weights[:, feature_idx].reshape(28, 28)\n",
        "    \n",
        "    # Normalize for visualization\n",
        "    feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min())\n",
        "    \n",
        "    axes[row, col].imshow(feature_map, cmap='RdBu', interpolation='nearest')\n",
        "    axes[row, col].set_title(f'Feature {feature_idx}', fontsize=10)\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Training metrics summary\n",
        "print(\"\\n📊 Training Summary:\")\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
        "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Generalization Gap: {final_train_acc - test_accuracy:.4f}\")\n",
        "\n",
        "# 4. Model efficiency metrics\n",
        "total_params = model.count_params()\n",
        "trainable_params = sum([np.prod(layer.get_weights()[0].shape) for layer in model.layers if layer.get_weights()])\n",
        "\n",
        "print(f\"\\n🔧 Model Efficiency:\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "print(f\"Model Size (approx): {total_params * 4 / (1024*1024):.2f} MB\")  # Assuming float32\n",
        "print(f\"Parameters per accuracy point: {total_params / test_accuracy:.0f}\")\n",
        "\n",
        "print(\"✅ Advanced model analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Alternative: Simple Model Compilation (If Above Fails)\n",
        "\n",
        "If you encounter metric errors, you can use this simpler version:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALTERNATIVE: Simple compilation (uncomment if needed)\n",
        "\"\"\"\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "print(\"✅ Model compiled with basic metrics\")\n",
        "\"\"\"\n",
        "\n",
        "# Calculate top-3 accuracy manually if needed\n",
        "def calculate_top_k_accuracy(y_true, y_pred, k=3):\n",
        "    \"\"\"Calculate top-k accuracy manually\"\"\"\n",
        "    y_true_labels = np.argmax(y_true, axis=1)\n",
        "    top_k_pred = np.argsort(y_pred, axis=1)[:, -k:]\n",
        "    \n",
        "    correct = 0\n",
        "    for i, true_label in enumerate(y_true_labels):\n",
        "        if true_label in top_k_pred[i]:\n",
        "            correct += 1\n",
        "    \n",
        "    return correct / len(y_true_labels)\n",
        "\n",
        "print(\"📝 Alternative compilation method and manual top-k calculation ready if needed\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
